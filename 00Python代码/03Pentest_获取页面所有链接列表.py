#!/usr/bin/python
#coding:utf-8

import requests
import re
import datetime
import random
from bs4 import BeautifulSoup as BS

pages=set()
random.seed(datetime.datetime.now())

#获取页面所有内链的列表
def getInternallinks(bsobj,includeurl):
	internallinks=[]
	#找出所有以'/'开头的链接
	for link in bsobj.findAll('a',href=re.compile('(/|.*'+includeurl+')')):
		if link.attrs['href'] is not None:
			if link.attrs['href'] not in internallinks:
				internallinks.append(link.attrs['href'])
	return internallinks


#获取页面所有外链的列表
def getExternallinks(bsobj,excludeurl):
	externallinks=[]
	#找出所有以'http'或'www'开头且不包含当前url的链接
	for link in bsobj.findAll('a',href=re.compile('^(http|www)((?!'+excludeurl+').)*$')):
		if link.attrs['href'] is not None:
			if link.attrs['href'] not in externallinks:
				externallinks.append(link.attrs['href'])
	return externallinks

def splitAddress(address):
	try:
		addressParts=address.replace('http://','').split('/')
		return addressParts
	except requests.exceptions,e:
		print e
		return address.replace('//','').split('/')


def getRandomExternallinks(startingPage):
	html=requests.get(startingPage)
	bsobj=BS(html.content,'lxml')
	externallinks=getExternallinks(bsobj,splitAddress(startingPage)[0])
	if len(externallinks) == 0:
		internallinks=getInternallinks(bsobj,startingPage)
		return getNextExternallink(internallinks[random.randint(0,len(internallinks)-1)])
	else:
		return externallinks[random.randint(0,len(externallinks)-1)]

def followExternalOnly(startingSite):
	externallink=getRandomExternallinks(startingSite)
	print '随机外链是：'+externallink
	followExternalOnly(externallink)

#followExternalOnly('http://zhibo8.cc')

#收集网站上发现的所有外链列表
allExtLinks=set()
allIntLinks=set()
def getAllExternallinks(siteurl):
	html=requests.get(siteurl)
	bsobj=BS(html.content,'lxml')
	internallinks=getInternallinks(bsobj,splitAddress(siteurl)[0])
	externallinks=getExternallinks(bsobj,splitAddress(siteurl)[0])

	for link in externallinks:
		if link not in allExtLinks:
			allExtLinks.add(link)
			print link
	for link in internallinks:
		if link not in allIntLinks:
			print '即将获取链接的URL是：'+link
			allIntLinks.add(link)
			comp=re.compile('^http:')
			try:	
				if re.findall(comp,link):
					print link
					getAllExternallinks(link)
				else:
					getAllExternallinks('http:'+link)
			except requests.exceptions.MissingSchema:
				print link
				print '----------MissingSchema-------------'
			except requests.exceptions.InvalidURL:
				print link
				print '----------InvalidURL----------------'
			except requests.exceptions.SSLError:
				print link
				print '-----------SSLError-----------------'

getAllExternallinks('http://speed.yn.chinamobile.com')
