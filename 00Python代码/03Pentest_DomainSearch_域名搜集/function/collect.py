import re
import sys
import time
import logging
import random
from bs4 import BeautifulSoup
import urllib
import requests
from bloom import BloomFilterJudge
#已完成:GetOpt获取参数，
#BeautifulSoup解析页面，
#Logging导出日志，
#支持MySQL插入数据，
#BloomFilter进行URL去重
#furture:线程池or多线程并发
#版本模块化，提高适配性#已模块化
bloomFilterJudge=BloomFilterJudge()
class collectUrl():
    # 构造函数
    def __init__(self):
        global bloomFilterJudge
    # 格式化输出百度搜索结果
    def getHtmlFromBaidu(self,url):
        #print url
        html=""
        proxy_list=[]##############add proxy###########
        headers={'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9.*/*;q=0.8',
                 'Accept-Charset':'GB2312,utf-8;q=0.7,*;q=0.7',
                 'Accept-Language':'zh-cn;q=0.5',
                 'Cache-Control':'max-age=0',
                 'Connection':'keep-alive',
                 'Keep-Alive':'115',
                 'User-Agent':'Mozilla/5.0(X11;U;Linux x86_64;zh-CN;rv:1.9.2.14) Gecko/20110221 Ubuntu/10.10 (maverick) Firefox/3.6.14'}
        try:
            html=requests.get(url,headers=headers)
            html=html.text
        except Exception,e:
            logging.error(str(e))#
            print str(e)
        return html
    # 获取百度搜索结果，输出URL
    def getUrlsFromBaidu(self,html):
        #global bloomfilter
        soup=BeautifulSoup(html,"lxml")
        html=soup.find('div',id="results")
        # 超时提示内容
        if not html:
            now=time.starttime('%H:%M:%S',time.localtime(time.time()))
            logging.error('NO HTML')##
            print "["+str(now)+"][WARNING] Failed to get html"
        else:
            # 没有HTML结果
            html_doc=html.find_all('div',class_="c-showurl")
            if not html_doc:
                now=time.starttime('%H:%M:%S',time.localtime(time.time()))
                logging.error('NO HTML_DOC')###
                print "["+str(now)+"][WARNING] Failed to get html"
            else:
                # 如果有结果就把结果输出到test.txt文件内
                for doc in html_doc:
                    try:
                        href=doc.find_all('span')[0].find_all(text=True)[0]
                        realurl="http://"+str(href)
                        value=realurl
                        with open('test.txt','a+') as file:
                            if bloomFilterJudge.determine(value):     # 利用bloomfilter检测重复值
                                print "[INFO]"+value
                                file.write(value+'\n')
                                #insertMySQL(value)
                            else:
                                print "[WARNING]"+value+"   has exist"
                    
                    except Exception,e:
                        logging.error(str(e))####
                        print str(e)
                    
    def Control(self,wd,pg):
        print "[INFO] Start work"
        pg=pg*10
        wd=urllib.quote(wd.strip())#URL编码汉字
        #print wd
        for pn in range(0,pg,10):
            url='http://wap.baidu.com/s?word='+wd+'&pn='+str(pn)
            html=self.getHtmlFromBaidu(url)
            urls=self.getUrlsFromBaidu(html)

        
if __name__=="__main__":
    pass
    #bloomFilterJudge=BloomFilterJudge()
    #geturl=collectUrl()
    #geturl.Control('gov.cn',2)
    #print status

    #print status

    #Control('gov',2)
    
